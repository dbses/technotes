# 38 | 架构师应该如何判断技术演进的方向？

面对层出不穷的新技术，我们应该采取什么样的策略？

1. 潮流派

潮流派的典型特征就是对于新技术特别热衷，紧跟技术潮流，当有新的技术出现时，迫切想将新的技术应用到自己的产品中。

首先，新技术需要时间成熟，如果刚出来就用，此时新技术还不怎么成熟，实际应用中很可能遇到各种“坑”，自己成了实验小白鼠。

其次，新技术需要学习，需要花费一定的时间去掌握，这个也是较大的成本；如果等到掌握了技术后又发现不适用，则是一种较大的人力浪费。

2. 保守派

对于新技术抱有很强的戒备心，稳定压倒一切，已经掌握了某种技术，就一直用这种技术打天下。

保守派的主要问题是不能享受新技术带来的收益，例如有了拖拉机，你还偏偏要用牛车。

3. 跟风派

跟着竞争对手的步子走。简单来说，判断技术的发展就看竞争对手，竞争对手用了咱们就用，竞争对手没用咱们就等等看。

跟风派最大的问题在于如果没有风可跟的时候怎么办。即使有风可跟，有时候适用于竞争对手的技术，并不一定适用于自己，盲目模仿可能带来相反的效果。另外，竞争对手的这些信息并不那么容易获取。

既然潮流派、保守派、跟风派都存在这样或者那样的问题，那架构师究竟如何判断技术演进的方向呢？

**技术演进的动力**

归根到底就是业务的发展。影响一个企业业务的发展主要有 3 个因素：市场、技术、管理，这三者构成支撑业务发展的铁三角，任何一个因素的不足，都可能导致企业的业务停滞不前。

<img src="https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210208215828.png" alt="image-20210208215828707" style="zoom: 50%;" />

我们可以简单地将企业的业务分为两类：一类是产品类，一类是服务类。

- 对于产品类业务，技术创新推动业务发展

用户选择一个产品的根本驱动力在于产品的功能是否能够更好地帮助自己完成任务。用户会自然而然地选择那些功能更加强大、性能更加先进、体验更加顺畅、外观更加漂亮的产品，而功能、性能、体验、外观等都需要强大的技术支撑。例如，iPhone 手机的多点触摸操作、UC 浏览器的 U3 内核等。

- 对于“服务”类的业务，业务发展推动技术的发展

用户选择一个产品的根本驱动力是其“功能”，而用户选择一个服务的根本驱动力不是功能，而是“规模”。例如，选择 UC 浏览器还是选择 QQ 浏览器，更多的人是根据个人喜好和体验来决定的；而选择微信还是 Whatsapp，是根据其规模来选择的。

服务类的业务发展路径是这样的：提出一种创新的服务模式→吸引了一批用户→业务开始发展→吸引了更多用户→服务模式不断完善和创新→吸引越来越多的用户，如此循环往复。

在这个发展路径中，技术并没有成为业务发展的驱动力，反过来由于用户规模的不断扩展，业务的不断创新和改进，对技术会提出越来越高的要求，因此是业务驱动了技术发展。

其实回到产品类业务，如果我们将观察的时间拉长来看，即使是产品类业务，在技术创新开创了一个新的业务后，后续的业务发展也会反向推动技术的发展。例如，第一代 iPhone 缺少对 3G 的支持，且只能通过 Web 发布应用程序，第二代 iPhone 才开始支持 3G，并且内置 GPS。

综合这些分析，除非是开创新的技术能够推动或者创造一种新的业务，其他情况下，都是业务的发展推动了技术的发展。

# 39 | 互联网技术演进的模式

以互联网的业务发展为案例，谈谈互联网技术演进的模式，其他行业可以参考分析方法对自己的行业进行分析。

互联网业务发展一般分为几个时期：初创期、发展期、竞争期、成熟期。不同时期的差别主要体现在两个方面：复杂性、用户规模。

**业务复杂性**

1. 初创期

互联网业务刚开始一般都是一个创新的业务点，这个业务点的重点不在于“完善”，而在于“创新”，只有创新才能吸引用户。

初创期的业务对技术就一个要求：“快”。

2. 发展期

当业务推出后经过市场验证如果是可行的，则吸引的用户就会越来越多，此时原来不完善的业务就进入了一个快速发展的时期。

这个阶段技术的核心工作是快速地实现各种需求。如何做到“快”，一般会经历下面几个阶段。

- 堆功能期

  业务进入快速发展期的初期，此时团队规模也不大，业务需求又很紧，最快实现业务需求的方式是继续在原有的系统里面不断地增加新的功能。

- 优化期

  随着功能越来越多，系统开始变得越来越复杂，后面继续堆功能会感到越来越吃力，速度越来越慢。

  如何解决这个问题，一般会分为两派：一派是优化派，一派是架构派。

- 架构期

  如果业务能够继续发展，慢慢就会发现优化也顶不住了，毕竟再怎么优化，系统的能力总是有极限的。

3. 竞争期

当业务继续发展，已经形成一定规模后，一定会有竞争对手开始加入行业来竞争。新业务的创新给技术带来的典型压力就是新的系统会更多，系统数量的量变带来了技术工作的质变。主要体现在下面几个方面：

- 重复造轮子

  系统越来越多，各系统相似的工作越来越多。

- 系统交互一团乱麻

  系统越来越多，各系统的交互关系变成了网状。

针对这个时期业务变化带来的问题，技术工作主要的解决手段有：

- 平台化

  目的在于解决“重复造轮子”的问题。

- 服务化

  目的在于解决“系统交互”的问题，常见的做法是通过消息队列来完成系统间的异步通知，通过服务框架来完成系统间的同步调用。

4. 成熟期

当企业熬过竞争期，成为了行业的领头羊，此时求快求新已经没有很大空间，业务上开始转向为“求精”。

**用户规模**

互联网业务的发展会经历“初创期、发展期、竞争期、成熟期”几个阶段，不同阶段典型的差别就是用户量的差别，用户量随着业务的发展而越来越大。

用户量增大对技术的影响主要体现在两个方面：性能要求越来越高、可用性要求越来越高。

1. 性能

用户量增大，性能要求也越高。以 MySQL 为例，单台 MySQL 机器支撑的 TPS 和 QPS 最高也就是万级，必然要考虑使用多台 MySQL。

2. 可用性

1 万用户宕机 1 小时，你可能才损失了几千元；100 万用户宕机 10 分钟，损失可能就是几十万元了。

**量变到质变**

互联网业务驱动技术发展的两大主要因素是复杂性和用户规模，这两个因素的本质其实都是“量变带来质变”。

![image-20210209223846189](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210209223846.png)

# 40 | 互联网架构模板：“存储层”技术

互联网的标准技术架构如下图所示。

<img src="https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210211075224.png" alt="image-20210211075224419" style="zoom:50%;" />

> 这张图基本上涵盖了互联网技术公司的大部分技术点，不同的公司只是在具体的技术实现上稍有差异，但不会跳出这个框架的范畴

从本期开始，我将逐层介绍每个技术点的产生背景、应用场景、关键技术。今天我们首先来聊聊互联网架构模板的“存储层”技术。

**SQL**

随着互联网业务的发展，性能要求越来越高，必然要面对一个问题：将数据拆分到多个数据库实例才能满足业务的性能需求。

数据库拆分满足了性能的要求，但带来了复杂度的问题：数据如何拆分、数据如何组合？这个复杂度的问题解决起来并不容易，如果每个业务都去实现一遍，重复造轮子将导致投入浪费、效率降低，业务开发想快都快不起来。

所以互联网公司流行的做法是业务发展到一定阶段后，就会将这部分功能独立成中间件。

如果公司业务继续发展，规模继续扩大，SQL 服务器越来越多，实力雄厚的大公司此时一般都会在 SQL 集群上构建 SQL 存储平台，以对业务透明的形式提供资源分配、数据备份、迁移、容灾、读写分离、分库分表等一系列服务。

**NoSQL**

如果公司发展很快，例如 Memcache 的节点有上千甚至几千时，通常都会在 NoSQL 集群的基础之上再实现统一存储平台，统一存储平台主要实现这几个功能：

- 资源按需动态分配：例如同一台 Memcache 服务器，可以根据内存利用率，分配给多个业务使用。
- 资源自动化管理：例如新业务只需要申请多少 Memcache 缓存空间就可以了，无需关注具体是哪些 Memcache 服务器在为自己提供服务。
- 故障自动化处理：例如某台 Memcache 服务器挂掉后，有另外一台备份 Memcache 服务器能立刻接管缓存请求，不会导致丢失很多缓存数据。

**小文件存储**

除了关系型的业务数据，互联网行业还有很多用于展示的数据。例如，淘宝的商品图片。这些数据具有三个典型特征：

一是数据小，一般在 1MB 以下；

二是数量巨大，Facebook 在 2013 年每天上传的照片就达到了 3.5 亿张；

三是访问量巨大，Facebook 每天的访问量超过 10 亿。

得益于开源运动的发展和最近几年大数据的火爆，HBase、Hadoop、Hypertable、FastDFS 等都可以作为小文件存储的底层平台。只需要将这些开源方案再包装一下基本上就可以用了。

典型的小文件存储有：淘宝的 TFS、京东 JFS、Facebook 的 Haystack。下图是淘宝 TFS 的架构：

![image-20210211080852748](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210211080852.png)

**大文件存储**

互联网行业的大文件主要分为两类：一类是业务上的大数据，例如 Youtube 的视频；另一类是海量的日志数据，例如各种访问日志。

说到大文件，特别要提到 Google 和 Yahoo，Google 的 3 篇大数据论文（Bigtable/Map- Reduce/GFS）开启了一个大数据的时代，而 Yahoo 开源的 Hadoop 系列（HDFS、HBase 等），基本上垄断了开源界的大数据处理。

下面是 Hadoop 的生态圈：

![image-20210211075037106](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210211075037.png)

**思考题**

既然存储技术发展到最后都是存储平台，为何没有出现存储平台的开源方案，但云计算却都提供了存储平台方案？

# 41 | 互联网架构模板：“开发层”和“服务层”技术

今天我们来聊聊互联网架构模板的“开发层”和“服务层”技术。

**开发层技术**

1. 开发框架

互联网公司都会指定一个大的技术方向，然后使用统一的开发框架。例如，Java 相关的开发框架 SSH、SpringMVC、Play，Ruby 的 Ruby on Rails，PHP 的 ThinkPHP，Python 的 Django 等。

对于框架的选择，有一个总的原则：优选成熟的框架，避免盲目追逐新技术！

2. Web 服务器

开发框架只是负责完成业务功能的开发，真正能够运行起来给用户提供服务，还需要服务器配合。

选择一个服务器主要和开发语言相关，例如，Java 的有 Tomcat、JBoss、Resin 等，PHP/Python 的用 Nginx，当然最保险的就是用 Apache 了，什么语言都支持。

3. 容器

传统的虚拟化技术是虚拟机，解决了跨平台的问题，但由于虚拟机太庞大，启动又慢，运行时太占资源，在互联网行业并没有大规模应用。

Docker 的容器技术，将在很大程度上改变目前的技术形势：

- 运维方式会发生革命性的变化：Docker 启动快，几乎不占资源，随时启动和停止，基于 Docker 打造自动化运维、智能化运维将成为主流方式。
- 设计模式会发生本质上的变化：启动一个新的容器实例代价如此低，将鼓励设计思路朝“微服务”的方向发展。

**服务层技术**

服务层的主要目标其实就是为了降低系统间相互关联的复杂度。

1. 配置中心

将配置中心做成通用的系统的好处有：

- 集中配置多个系统，操作效率高。所有配置都在一个集中的地方，检查方便，协作效率高。
- 配置中心可以实现程序化的规则检查，避免常见的错误。比如说检查最小值、最大值、是否 IP 地址、是否 URL 地址，都可以用正则表达式完成。
- 配置中心相当于备份了系统的配置，当某些情况下需要搭建新的环境时，能够快速搭建环境和恢复业务。

下面是配置中心简单的设计，其中通过“系统标识 + host + port”来标识唯一一个系统运行实例是常见的设计方法。

<img src="https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210213184508.png" alt="image-20210213184508579" style="zoom:50%;" />

2. 服务中心

当系统数量不多的时候，系统间的调用一般都是直接通过配置文件记录在各系统内部的，但当系统数量多了以后，这种方式就存在问题了。

比如说总共有 10 个系统依赖 A 系统的 X 接口，A 系统实现了一个新接口 Y，能够更好地提供原有 X 接口的功能，如果要让已有的 10 个系统都切换到 Y 接口，则这 10 个系统的几十上百台机器的配置都要修改，然后重启，可想而知这个效率是很低的。

除此以外，如果 A 系统总共有 20 台机器，现在其中 5 台出故障了，其他系统如果是通过域名访问 A 系统，则域名缓存失效前，还是可能访问到这 5 台故障机器的。

如果其他系统通过 IP 访问 A 系统，那么 A 系统每次增加或者删除机器，其他所有 10 个系统的几十上百台机器都要同步修改，这样的协调工作量也是非常大的。

服务中心就是为了解决上面提到的跨系统依赖的“配置”和“调度”问题。一般来说有两种实现方式：服务名字系统和服务总线系统。

- 服务名字系统（Service Name System）

服务名字系统跟 DNS 性质基本是类似的。服务名字系统是为了将 Service 名称解析为“host + port + 接口名称”，但是和 DNS 一样，真正发起请求的还是请求方。

基本的设计如下：

![image-20210213204704102](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210213204916.png)

- 服务总线系统（Service Bus System）

服务总线系统跟计算机总线本质也是基本类似的。相比服务名字系统，服务总线系统更进一步了：由总线系统完成调用，服务请求方都不需要直接和服务提供方交互了。

基本的设计如下：

![image-20210213205035362](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210213205035.png)

“服务名字系统”和“服务总线系统”简单对比如下表所示。

![image-20210213205109188](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210213205207.png)

3. 消息队列

互联网业务的一个特点是“快”，这就要求很多业务处理采用异步的方式。例如，大 V 发布一条微博后，系统需要发消息给关注的用户，我们不可能等到所有消息都发送给关注用户后再告诉大 V 说微博发布成功了，只能先让大 V 发布微博，然后再发消息给关注用户。

传统的异步通知方式是由消息生产者直接调用消息消费者提供的接口进行通知的，但当业务变得庞大，子系统数量增多时，这样做会导致系统间交互非常复杂和难以管理，因为系统间互相依赖和调用，整个系统的结构就像一张蜘蛛网，如下图所示。

![image-20210213205415948](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210213205431.png)

消息队列就是为了实现这种跨系统异步通知的中间件系统。消息队列既可以“一对一”通知，也可以“一对多”广播。以微博为例，可以清晰地看到异步通知的实现和作用，如下图所示。

![image-20210213205431733](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210213205431.png)

# 42 | 互联网架构模板：“网络层”技术

当我们站在一个公司的的角度来思考架构的时候，单个系统的高可用和高性能并不等于整体业务的高可用和高性能，互联网业务的高性能和高可用需要从更高的角度去设计，这个高点就是“网络”。

**负载均衡**

每个系统的处理能力是有限的，为了应对大容量的访问，必须使用多个系统。

1. DNS

DNS 一般用来实现地理级别的均衡。例如，北方的用户访问北京的机房，南方的用户访问广州的机房。

DNS 负载均衡的优点是通用（全球通用）、成本低（申请域名，注册 DNS 即可），但缺点也比较明显，主要体现在：

- 缓存的时间比较长
- 没有灵活的负载均衡策略

2. Nginx 、LVS 、F5

Nginx、LVS、F5 用于同一地点内机器级别的负载均衡。其中 Nginx 是软件的 7 层负载均衡，LVS 是内核的 4 层负载均衡，F5 是硬件的 4 层负载均衡。

软件和硬件的区别就在于性能，硬件远远高于软件，Ngxin 的性能是万级，一般的 Linux 服务器上装个 Nginx 大概能到 5 万 / 秒；LVS 的性能是十万级，据说可达到 80 万 / 秒；F5 性能是百万级，从 200 万 / 秒到 800 万 / 秒都有。

**CDN**

CDN 是为了解决用户网络访问时的“最后一公里”效应，本质上是一种“以空间换时间”的加速策略，即将内容缓存在离用户最近的地方，用户访问的是缓存的内容，而不是站点实时的内容。

![image-20210219221045289](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210219221045.png)

目前有专门的 CDN 服务商，例如网宿和蓝汛；也有云计算厂家提供 CDN 服务，例如阿里云和腾讯云都提供 CDN 的服务。

**多机房**

多机房设计最核心的因素就是如何处理时延带来的影响，常见的策略有：

1. 同城多机房

同一个城市多个机房，距离不会太远，可以投入重金，搭建私有的高速网络，基本上能够做到和同机房一样的效果。

这种方式对业务影响很小，但投入较大，如果不是大公司，一般是承受不起的；而且遇到极端的地震、水灾等自然灾害，同城多机房也是有很大风险的。

2. 跨城多机房

在不同的城市搭建多个机房，机房间通过网络进行数据复制（例如，MySQL 主备复制），但由于跨城网络时延的问题，业务上需要做一定的妥协和兼容，比如不需要数据的实时强一致性，只是保证最终一致性。

例如，微博类产品，B 用户关注了 A 用户，A 用户在北京机房发布了一条微博，B 在广州机房不需要立刻看到 A 用户发的微博，等 10 分钟看到也可以。

3. 跨国多机房

和跨城多机房类似，只是地理上分布更远，时延更大。由于时延太大和用户跨国访问实在太慢，跨国多机房一般仅用于备份和服务本国用户。

**多中心**

简单来说，多机房的主要目标是灾备，当机房故障时，可以比较快速地将业务切换到另外一个机房，这种切换操作允许一定时间的中断（例如，10 分钟、1 个小时），而且业务也可能有损失（例如，某些未同步的数据不能马上恢复，或者要等几天才恢复，甚至永远都不能恢复了）。

相比多机房来说，多中心的要求就高多了，要求每个中心都同时对外提供服务，且业务能够自动在多中心之间切换，故障后不需人工干预或者很少的人工干预就能自动恢复。

# 43 | 互联网架构模板：“用户层”和“业务层”技术

今天，我将从“用户层”和“业务层”的角度谈谈常见的应用场景和关键技术。

**用户层技术**

1. 用户管理

用户管理的第一个目标：单点登录（SSO），用户管理的第二个目标：授权登录。

2. 消息推送

消息推送主要包含 3 个功能：设备管理（唯一标识、注册、注销）、连接管理和消息管理，技术上面临的主要挑战有：

- 海量设备和用户管理﻿
- 连接保活﻿
- 消息管理﻿

3. 存储云、图片云

互联网业务场景中，用户会上传多种类型的文件数据，这些文件具备几个典型特点：

- 数据量大
- 文件体积小
- 访问有时效性

**业务层技术**

业务层面对的主要技术挑战是“复杂度”。复杂度越来越高的一个主要原因就是系统越来越庞大，业务越来越多。

解决业务复杂度的能用方法是化整为零、分而治之，将整体复杂性分散到多个子业务或者子系统里面去。

随着子系统数量越来越多，如果达到几百上千，出了问题排查也会特别复杂。

正所谓“合久必分、分久必合”，但合的方式不一样，此时采取的“合”的方式是按照“高内聚、低耦合”的原则。同样以电商为样例，采用虚拟业务域后，其架构如下：

<img src="https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210220215610.png" alt="image-20210220215610172" style="zoom:50%;" />

# 44 | 互联网架构模板：“平台”技术

当业务规模比较小、系统复杂度不高时，运维、测试、数据分析、管理等支撑功能主要由各系统或者团队独立完成。

随着业务规模越来越大，如果继续采取各自为政的方式来实现这些支撑功能，会发现重复工作非常多。

**运维平台**

运维平台核心的职责分为四大块：配置、部署、监控、应急。

运维平台的核心设计要素是“四化”：标准化、平台化、自动化、可视化。

1. 标准化

需要制定运维标准，规范配置管理、部署流程、监控指标、应急能力等。

2. 平台化

运维平台的好处有：

- 可以将运维标准固化到平台中，无须运维人员死记硬背运维标准。
- 运维平台提供简单方便的操作，相比之下人工操作低效且容易出错。
- 运维平台是可复用的，一套运维平台可以支撑几百上千个业务系统。

3. 自动化

传统手工运维方式效率低下的一个主要原因就是要执行大量重复的操作，运维平台可以将这些重复操作固化下来，由系统自动完成。

4. 可视化

可视化相比简单的数据罗列，具备下面这些优点：

- 能够直观地看到数据的相关属性，例如，汽车仪表盘中的数据最小值是 0，最大是 100，单位是 MPH。
- 能够将数据的含义展示出来，例如汽车仪表盘中不同速度的颜色指示。
- 能够将关联数据整合一起展示，例如汽车仪表盘的速度和里程。

**测试平台**

为了达到“自动化”的目标，测试平台的基本架构如下图所示。

<img src="https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210220221605.png" alt="image-20210220221605408" style="zoom:50%;" />

1. 用例管理

测试自动化的主要手段就是通过脚本或者代码来进行测试。

2. 资源管理

测试用例要放到具体的运行环境中才能真正执行，运行环境包括硬件（服务器、手机、平板电脑等）、软件（操作系统、数据库、Java 虚拟机等）、业务系统（被测试的系统）。

3. 任务管理

任务管理的主要职责是将测试用例分配到具体的资源上执行，跟踪任务的执行情况。任务管理是测试平台设计的核心，它将测试平台的各个部分串联起来从而完成自动化测试。

4. 数据管理

测试任务执行完成后，需要记录各种相关的数据（例如，执行时间、执行结果、用例执行期间的 CPU、内存占用情况等）。

**数据平台**

数据平台的核心职责主要包括三部分：数据管理、数据分析和数据应用。每一部分又包含更多的细分领域，详细的数据平台架构如下图所示。

<img src="https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210220221901.png" alt="image-20210220221901841" style="zoom:50%;" />

1. 数据管理

数据管理包含数据采集、数据存储、数据访问和数据安全四个核心职责，是数据平台的基础功能。

2. 数据分析

数据分析包括数据统计、数据挖掘、机器学习、深度学习等几个细分领域。

3. 数据应用

数据应用很广泛，既包括在线业务，也包括离线业务。例如，推荐、广告等属于在线应用，报表、欺诈检测、异常检测等属于离线应用。

**管理平台**

权限管理主要分为两部分：身份认证、权限控制，其基本架构如下图所示。

<img src="https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210220222009.png" alt="image-20210220222009109" style="zoom:50%;" />

1. 身份认证

确定当前的操作人员身份，防止非法人员进入系统。例如，不允许匿名用户进入系统。为了避免每个系统都自己来管理用户，通常情况下都会使用企业账号来做统一认证和登录。

2. 权限控制

根据操作人员的身份确定操作权限，防止未经授权的人员进行操作。例如，不允许研发人员进入财务系统查看别人的工资。

# 45 | 架构重构内功心法第一式：有的放矢

相比全新的架构设计来说，架构重构对架构师的要求更高，主要体现在：

- 业务已经上线，不能停下来
- 关联方众多，牵一发动全身
- 旧架构的约束

接下来我将分 3 期传授我的架构重构内功心法，今天先来看第一式：有的放矢。

期望通过架构重构来解决所有问题当然是不现实的，所以架构师的首要任务是从一大堆纷繁复杂的问题中识别出真正要通过架构重构来解决的问题，集中力量快速解决。避免摊大饼式或者运动式的重构和优化。

我们来看几个具体的重构案例。

1. 后台系统重构：解决不合理的耦合

举一个简单的例子：数据库中的某张表，一部分字段是所有业务公用的“游戏数据”，一部分字段是 P 业务系统“独有的数据”，开发时如果要改这张表，代码和逻辑都很复杂，改起来效率很低。

![image-20210221213509126](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210221213509.png)

针对 M 系统存在的问题，重构目标就是将游戏数据和业务数据拆分，解开两者的耦合，使得两个系统都能够独立快速发展。重构的方案如下图所示。

![image-20210221213537734](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210221213537.png)

重构后的效果非常明显，重构后的 M 系统和 P 业务后台系统每月上线版本数是重构前的 4 倍！

2. 游戏接入系统重构：解决全局单点的可用性问题

S 系统是游戏接入的核心系统，一旦 S 系统故障，大量游戏玩家就不能登录游戏。其大概架构如下图所示，可以看出数据库主库是全局单点，一旦数据库主库不可用，两个集群的写业务都不可用了。

![image-20210221213803114](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210221213803.png)

针对 S 系统存在的问题，重构目标就是实现双中心，使得任意一个机房都能够提供完整的服务，在某个机房故障时，另外一个机房能够全部接管所有业务。重构方案如下图所示。

![image-20210221213905272](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210221213905.png)

重构后系统的可用性从 3 个 9 提升到 4 个 9，重构前最夸张的一个月有 4 次较大的线上故障，重构后虽然也经历了机房交换机宕机、运营商线路故障、机柜断电等问题，但对业务都没有什么大的影响。

3. X 系统：解决大系统带来的开发效率问题

X 系统是创新业务的主系统，之前在业务快速尝试和快速发展期间，怎么方便怎么操作，怎么快速怎么做，系统设计并未投入太多精力和时间，很多东西都“塞”到同一个系统中，导致到了现在已经改不动了。做一个新功能或者新业务，需要花费大量的时间来讨论和梳理各种业务逻辑，一不小心就踩个大坑。X 系统的架构如下图所示。

![image-20210221214646697](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210221214646.png)

针对 X 系统存在的问题，重构目标是将各个功能拆分到不同的子系统中，降低单个系统的复杂度。重构后的架构如下图所示（仅仅是示例，实际架构远比下图复杂）。

![image-20210221214705591](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210221214705.png)

重构后各个系统之间通过接口交互，虽然看似增加了接口的工作量，但整体来说，各系统的发展和开发速度比原来快了很多，系统也相对更加简单，也不会出现某个子系统有问题，所有业务都有问题。

这三个系统重构的方案，现在回过头来看，感觉是理所当然的，但实际上当时做分析和决策时，远远没有这么简单。以 M 系统为例，当时我们接手后遇到的问题有很多，例如：

- 数据经常出错。
- M 系统是单机，单机宕机后所有后台操作就不能进行了。
- 性能比较差，有的操作耗时好久。
- 界面比较丑，操作不人性化。
- 历史上经过几手转接，代码比较混乱。
- 业务数据和游戏数据耦合，开发效率很低。

从这么多问题中识别出重构的目标，并不是一目了然的；而如果想一下全部解决所有这些问题，人力和时间又不够！所以架构师需要透过问题表象看到问题本质，找出真正需要通过架构重构解决的核心问题，从而做到有的放矢。

# 46 | 架构重构内功心法第二式：合纵连横

上一期我给你讲了我的架构重构内功心法的第一式：有的放矢，需要架构师透过问题表象看到问题本质，找出真正需要通过架构重构解决的核心问题，而不是想着通过一次重构解决所有问题。

今天我来传授架构重构内功心法的第二式：合纵连横。

**合纵**

要想真正推动一个架构重构项目启动，需要花费大量的精力进行游说和沟通。在沟通协调时，将技术语言转换为通俗语言，以事实说话，以数据说话，是沟通的关键！

以专栏上一期]的 M 系统为例，我们把“可扩展性”转换为“版本开发速度很慢，每次设计都要考虑是否对门户有影响，是否要考虑对其他业务有影响”，然后我们还收集了 1 个月里的版本情况，发现有几个版本设计阶段讨论 1 周甚至 2 周时间，但开发只有 2 天时间；而且一个月才做了 4 个版本，最极端的一个版本讨论 2 周，开发 2 天，然后等了 1 个月才和门户系统一起上线，项目经理和产品经理一听都被吓到了。

以上一期]的 S 系统为例，我们并没有直接说可用性是几个 9，而是整理线上故障的次数、每次影响的时长，影响的用户，客服的反馈意见等，然后再拿其他系统的数据进行对比，无论是产品人员、项目人员，还是运营人员，明显就看出系统的可用性有问题了。

**连横**

除了上面讨论的和上下游沟通协调，有的重构还需要和其他相关或者配合的系统的沟通协调。这部分的沟通协调其实相对来说要容易一些，但也不是说想推动就能推动的，主要的阻力来自“这对我有什么好处”和“这部分我这边现在不急”。

那如何才能有效地推动呢？有效的策略是“换位思考、合作双赢、关注长期”。简单来说就是站在对方的角度思考，重构对他有什么好处，能够帮他解决什么问题，带来什么收益。

# 47 | 架构重构内功心法第三式：运筹帷幄

架构师在识别系统关键的复杂度问题后，还需要识别为了解决这个问题，需要做哪些准备事项，或者还要先解决哪些问题。这就需要我今天要和你分享的架构重构内功心法第三式：运筹帷幄。

以 X 系统为例，在我加入前，其实也整理了系统目前存在的问题，大的项包括可用性、性能、安全、用户体验等，每个大项又包括十几二十个子项。但是实施时基本上就是挑软柿子捏，觉得哪个好落地、占用资源不太多，就挑来做，结果做了半年，好像做了很多功能，但整体却没什么进展。

后来我们成立了一个“X 项目”，在原来整理的问题基础上，识别出架构的核心复杂度体现在庞大的系统集成了太多功能，可扩展性不足；但目前系统的可用性也不高，经常出线上问题，耗费大量的人力去处理。因此我们又识别出如果要做架构重构，就需要系统处于一个比较稳定的状态，不要经常出线上问题。而目前系统的可用性不高，有的是因为硬件资源不够用了，或者某些系统组件使用不合理，有的是因为架构上存在问题。

基于这些分析，我们制定了总体的策略，如下图所示。

![image-20210222232204399](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210222232204.png)

可以看到，真正的架构重构在第三阶段，第一阶段和第二阶段都是为了第三阶段做准备而已，但如果没有第一阶段和第二阶段的铺垫，直接开始第三阶段的架构重构工作，架构重构方案需要糅合第一阶段和第二阶段的一些事项（例如，业务降级、接入服务中心等），会导致架构重构方案不聚焦，而且异常复杂。

总结一下重构的做法，其实就是“分段实施”，将要解决的问题根据优先级、重要性、实施难度等划分为不同的阶段，每个阶段聚焦于一个整体的目标，集中精力和资源解决一类问题。

具体如何制定“分段实施”的策略呢？

1. 优先级排序
2. 问题分类
3. 先易后难
4. 循序渐进

# 48 | 再谈开源项目：如何选择、使用以及二次开发？

**选：如何选择一个开源项目**

1. 聚焦是否满足业务

有的架构师在选择时有点无所适从，总是会担心选择了 A 项目而错过了 B 项目。这个问题的解决方式是聚焦于是否满足业务，而不需要过于关注开源项目是否优秀。

2. 聚焦是否成熟

可以从这几个方面考察开源项目是否成熟：

- 版本号：除非特殊情况，否则不要选 0.X 版本的，至少选 1.X 版本的，版本号越高越好。
- 使用的公司数量：一般开源项目都会把采用了自己项目的公司列在主页上，公司越大越好，数量越多越好。
- 社区活跃度：看看社区是否活跃，发帖数、回复数、问题处理速度等。

3. 聚焦运维能力

可以从这几个方面去考察运维能力：

- 开源项目日志是否齐全：有的开源项目日志只有寥寥启动停止几行，出了问题根本无法排查。
- 开源项目是否有命令行、管理控制台等维护工具，能够看到系统运行时的情况。
- 开源项目是否有故障检测和恢复的能力，例如告警、切换等。

**用：如何使用开源项目**

1. 深入研究，仔细测试

可以从这几方面进行研究和测试。

- 通读开源项目的设计文档或者白皮书，了解其设计原理。
- 核对每个配置项的作用和影响，识别出关键配置项。
- 进行多种场景的性能测试。
- 进行压力测试，连续跑几天，观察 CPU、内存、磁盘 I/O 等指标波动。
- 进行故障测试：kill、断电、拔网线、重启 100 次以上、切换等。

2. 小心应用，灰度发布

再怎么仔细地测试，都只能降低风险，但不可能完全覆盖所有线上场景。

3. 做好应急，以防万一

对于重要的业务或者数据，使用开源项目时，最好有另外一个比较成熟的方案做备份，尤其是数据存储。例如，如果要用 MongoDB 或者 Redis，可以用 MySQL 做备份存储。这样做虽然复杂度和成本高一些，但关键时刻能够救命！

**改：如何基于开源项目做二次开发**

1. 保持纯洁，加以包装

当我们发现开源项目有的地方不满足我们的需求时，自然会有一种去改改的冲动。这样做有几个比较严重的问题：

- 投入太大，一般来说，Redis 这种级别的开源项目，真要自己改，至少要投入 2 个人，搞 1 个月以上。
- 失去了跟随原项目演进的能力：改的太多，即使原有开源项目继续演进，也无法合并了，因为差异太大。

所以我的建议是不要改动原系统，而是要开发辅助系统：监控、报警、负载均衡、管理等。如果实在想改到原有系统，怎么办呢？我们的建议是直接给开源项目提需求或者 bug。

2. 发明你要的轮子

其实选与不选开源项目，核心还是一个成本和收益的问题，并不是说选择开源项目就一定是最优的项目，最主要的问题是：没有完全适合你的轮子！

