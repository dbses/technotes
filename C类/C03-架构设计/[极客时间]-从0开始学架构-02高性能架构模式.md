# 14 | 高性能数据库集群：读写分离

接下来，我将逐一介绍最常见的“高性能架构模式”“高可用架构模式”“可扩展架构模式”，这些模式可能你之前大概了解过，但其实每个方案里面都有很多细节，只有深入的理解这些细节才能理解常见的架构模式，进而设计出优秀的架构。

高性能数据库集群的第一种方式是“读写分离”，其本质是将访问压力分散到集群中的多个节点，但是没有分散存储压力；第二种方式是“分库分表”，既可以分散访问压力，又可以分散存储压力。先来看看“读写分离”。

**读写分离原理**

读写分离的基本原理是将数据库读写操作分散到不同的节点上，下面是其基本架构图。

<img src="https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210113230813.png" alt="image-20210113230813771" style="zoom:67%;" />

读写分离的基本实现是：

- 数据库服务器搭建主从集群，一主一从、一主多从都可以。
- 数据库主机负责读写操作，从机只负责读操作。
- 数据库主机通过复制将数据同步到从机，每台数据库服务器都存储了所有的业务数据。
- 业务服务器将写操作发给数据库主机，将读操作发给数据库从机。

需要注意的是，这里用的是“主从集群”，而不是“主备集群”。“从机”是需要提供读数据的功能的；而“备机”一般被认为仅仅提供备份功能，不提供访问功能。

读写分离的实现逻辑并不复杂，但有两个细节点将引入设计复杂度：主从复制延迟和分配机制。

**主从复制延迟**

以 MySQL 为例，主从复制延迟可能达到 1 秒，如果有大量数据同步，延迟 1 分钟也是有可能的。主从复制延迟会带来一个问题：如果业务服务器将数据写入到数据库主服务器后立刻（1 秒内）进行读取，此时读操作访问的是从机，主机还没有将数据复制过来，到从机读取数据是读不到最新数据的，业务上就可能出现问题。例如，用户刚注册完后立刻登录，业务服务器会提示他“你还没有注册”，而用户明明刚才已经注册成功了。

解决主从复制延迟有几种常见的方法：

1. 写操作后的读操作指定发给数据库主服务器

例如，注册账号完成后，登录时读取账号的读操作也发给数据库主服务器。这种方式和业务强绑定，对业务的侵入和影响较大。

2. 读从机失败后再读一次主机

这就是通常所说的“二次读取”，二次读取和业务无绑定，只需要对底层数据库访问的 API 进行封装即可，实现代价较小，不足之处在于如果有很多二次读取，将大大增加主机的读操作压力。例如，黑客暴力破解账号，会导致大量的二次读取操作，主机可能顶不住读操作的压力从而崩溃。

3. 关键业务读写操作全部指向主机，非关键业务采用读写分离

例如，对于一个用户管理系统来说，注册 + 登录的业务读写操作全部访问主机，用户的介绍、爱好、等级等业务，可以采用读写分离，因为即使用户改了自己的自我介绍，在查询时却看到了自我介绍还是旧的，业务影响与不能登录相比就小很多，还可以忍受。

**分配机制**

如何实现读写分离？一般有两种方式：程序代码封装和中间件封装。

1. 程序代码封装

程序代码封装指在代码中抽象一个数据访问层，实现读写操作分离和数据库服务器连接的管理。例如，基于 Hibernate 进行简单封装，就可以实现读写分离，基本架构是：

<img src="https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210113232258.png" alt="image-20210113232258068" style="zoom:67%;" />

程序代码封装的方式具备几个特点：

- 实现简单，而且可以根据业务做较多定制化的功能。
- 每个编程语言都需要自己实现一次，无法通用，如果一个业务包含多个编程语言写的多个子系统，则重复开发的工作量比较大。
- 故障情况下，如果主从发生切换，则可能需要所有系统都修改配置并重启。

实现方案有：淘宝的 TDDL（开源）。

2. 中间件封装

中间件封装指的是独立一套系统出来，实现读写操作分离和数据库服务器连接的管理。其基本架构是：

<img src="https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210113232837.png" alt="image-20210113232837631" style="zoom:67%;" />

数据库中间件的方式具备的特点是：

- 能够支持多种编程语言，因为数据库中间件对业务服务器提供的是标准 SQL 接口。
- 数据库中间件要支持完整的 SQL 语法和数据库服务器的协议（例如，MySQL 客户端和服务器的连接协议），实现比较复杂，细节特别多，很容易出现 bug，需要较长的时间才能稳定。
- 数据库中间件自己不执行真正的读写操作，但所有的数据库操作请求都要经过中间件，中间件的性能要求也很高。
- 数据库主从切换对业务服务器无感知，数据库中间件可以探测数据库服务器的主从状态。例如，向某个测试表写入一条数据，成功的就是主机，失败的就是从机。

实现方案有：MySQL Proxy、MySQL Router、奇虎 360 公司的Atlas。

# 15 | 高性能数据库集群：分库分表

读写分离分散了数据库读写操作的压力，但没有分散存储压力，当数据量达到千万甚至上亿条的时候，单台数据库服务器的存储能力会成为系统的瓶颈。

单个数据库服务器存储的数据量不能太大，需要控制在一定的范围内。为了满足业务数据存储的需求，就需要将存储分散到多台数据库服务器上。

常见的分散存储的方法是“分库分表”，其中包括“分库”和“分表”两大类。

**业务分库**

业务分库指的是按照业务模块将数据分散到不同的数据库服务器。例如，一个简单的电商网站，包括用户、商品、订单三个业务模块，我们可以将用户数据、商品数据、订单数据分开放到三台不同的数据库服务器上。

![image-20210115172545691](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/image-20210115172545691.png)

虽然业务分库能够分散存储和访问压力，但同时也带来了新的问题：

1. join 操作问题
2. 事务问题
3. 成本问题

**分表**

单表数据拆分有两种方式：垂直分表和水平分表。示意图如下：

![image-20210115172800951](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/image-20210115172800951.png)

分表能够有效地分散存储压力和带来性能提升，但和分库一样，也会引入各种复杂性。

1. 垂直分表

   垂直分表引入的复杂性主要体现在表操作的数量要增加。例如，原来只要一次查询就可以获取 name、age、sex、nickname、description，现在需要两次查询，一次查询获取 name、age、sex，另外一次查询获取 nickname、description。

2. 水平分表

   当看到表的数据量达到千万级别时，作为架构师就要警觉起来，因为这很可能是架构的性能瓶颈或者隐患。

水平分表相比垂直分表，会引入更多的复杂性，主要表现在下面几个方面：

1. 路由

   常见的路由算法有：范围路由、Hash 路由、配置路由（用一张独立的表来记录路由信息）。

2. join 操作

3. count() 操作

4. order by 操作

   由业务代码或者数据库中间件分别查询每个子表中的数据，然后汇总进行排序。

**实现方法**

和数据库读写分离类似，分库分表具体的实现方式也是“程序代码封装”和“中间件封装”，但实现会更复杂。

读写分离实现时只要识别 SQL 操作是读操作还是写操作，通过简单的判断 SELECT、UPDATE、INSERT、DELETE 几个关键字就可以做到。

分库分表的实现除了要判断操作类型外，还要判断 SQL 中具体需要操作的表、操作函数（例如 count 函数)、order by、group by 操作等，然后再根据不同的操作进行不同的处理。例如 order by 操作，需要先从多个库查询到各个库的数据，然后再重新 order by 才能得到最终的结果。

**思考题**

你认为什么时候引入分库分表是合适的？是数据库性能不够的时候就开始分库分表么？

答：依次尝试：1.做硬件优化；2.     做数据库服务器的调优；3.引入缓存技术；4.程序与数据库表优化；

在这些操作都不能大幅度优化性能的情况下，不能满足将来的发展，再考虑分库分表。

# 16 | 高性能NoSQL

关系数据库存在如下缺点：

- 关系数据库存储的是行记录，无法存储数据结构

- 关系数据库的 schema 扩展很不方便

- 关系数据库在大数据场景下 I/O 较高

  因为即使只针对其中某一列进行运算，关系数据库也会将整行数据从存储设备读入内存。

- 关系数据库的全文搜索功能比较弱

针对上述问题，分别诞生了不同的 NoSQL 解决方案，NoSQL 方案带来的优势，本质上是牺牲 ACID 中的某个或者某几个特性，常见的 NoSQL 方案分为 4 类：

- K-V 存储：解决关系数据库无法存储数据结构的问题，以 Redis 为代表。
- 文档数据库：解决关系数据库强 schema 约束的问题，以 MongoDB 为代表。
- 列式数据库：解决关系数据库大数据场景下的 I/O 问题，以 HBase 为代表。
- 全文搜索引擎：解决关系数据库的全文搜索性能问题，以 Elasticsearch 为代表。

今天，我来介绍一下各种高性能 NoSQL 方案的典型特征和应用场景。

**K-V 存储**

Redis 的 Value 是具体的数据结构，包括 string、hash、list、set、sorted set、bitmap 和 hyperloglog，所以常常被称为数据结构服务器。

Redis 的事务只能保证隔离性和一致性（I 和 C），无法保证原子性和持久性（A 和 D）。

虽然 Redis 并没有严格遵循 ACID 原则，但实际上大部分业务也不需要严格遵循 ACID 原则。以上面的微博关注操作为例，即使系统没有将 A 加入 B 的粉丝列表，其实业务影响也非常小，因此我们在设计方案时，需要根据业务特性和要求来确定是否可以用 Redis，而不能因为 Redis 不遵循 ACID 原则就直接放弃。

**文档数据库**

文档数据库最大的特点就是 no-schema，可以存储和读取任意的数据。文档数据库的 no-schema 特性，给业务开发带来了几个明显的优势。

- 新增字段简单
- 历史数据不会出错
- 可以很容易存储复杂数据

文档数据库的这个特点，特别适合电商和游戏这类的业务场景。以电商为例，不同商品的属性差异很大。例如，冰箱的属性和笔记本电脑的属性差异非常大，即使是同类商品也有不同的属性。例如，LCD 和 LED 显示器，两者有不同的参数指标。

这种业务场景如果使用关系数据库来存储数据，就会很麻烦，而使用文档数据库，会简单、方便许多，扩展新的属性也更加容易。

文档数据库 no-schema 的特性带来的这些优势也是有代价的，最主要的代价就是不支持事务。另外一个缺点就是无法实现关系数据库的 join 操作。

**列式数据库**

有些场景下，行式存储的优势将不复存在，甚至成为劣势，典型的场景就是海量数据进行统计。例如，计算某个城市体重超重的人员数据，实际上只需要读取每个人的体重这一列并进行统计即可，而行式存储即使最终只使用一列，也会将所有行数据都读取出来。

除了节省 I/O，列式存储还具备更高的存储压缩比，能够节省更多的存储空间。普通的行式数据库一般压缩率在 3:1 到 5:1 左右，而列式数据库的压缩率一般在 8:1 到 30:1 左右，因为单个列的数据相似度相比行来说更高，能够达到更高的压缩率。

同样，如果场景发生变化，列式存储的优势又会变成劣势。典型的场景是需要频繁地更新多个列。因为列式存储将不同列存储在磁盘上不连续的空间，导致更新多个列时磁盘是随机写操作；而行式存储时同一行多个列都存储在连续的空间，一次磁盘写操作就可以完成，列式存储的随机写效率要远远低于行式存储的写效率。

此外，列式存储高压缩率在更新场景下也会成为劣势，因为更新时需要将存储数据解压后更新，然后再压缩，最后写入磁盘。

基于上述列式存储的优缺点，一般将列式存储应用在离线的大数据分析和统计场景中，因为这种场景主要是针对部分列单列进行操作，且数据写入后就无须再更新删除。

**全文搜索引擎**

传统的关系型数据库在全文搜索的业务场景下，索引也无能为力，主要体现在：

- 全文搜索的条件可以随意排列组合，如果通过索引来满足，则索引的数量会非常多。
- 全文搜索的模糊匹配方式，索引无法满足，只能用 like 查询，而 like 查询是整表扫描，效率非常低。

全文搜索引擎的技术原理被称为“倒排索引”（Inverted index），是一种索引方法，其基本原理是建立单词到文档的索引。之所以被称为“倒排”索引，是和“正排“索引相对的，“正排索引”的基本原理是建立文档到单词的索引。

正排索引适用于根据文档名称来查询文档内容。例如，用户在网站上单击了“面向对象葵花宝典是什么”，网站根据文章标题查询文章的内容展示给用户。

倒排索引适用于根据关键词来查询文档内容。例如，用户只是想看“设计”相关的文章，网站需要将文章内容中包含“设计”一词的文章都搜索出来展示给用户。

为了让全文搜索引擎支持关系型数据的全文搜索，需要做一些转换操作，即将关系型数据转换为文档数据。

```json
{
	"id": 1,
	" 姓名 ": " 多隆 ",
	" 性别 ": " 男 ",
	" 地点 ": " 北京 ",
	" 单位 ": " 猫厂 ",
	" 爱好 ": " 写代码，旅游，马拉松 ",
	" 语言 ": "Java、C++、PHP",
	" 自我介绍 ": " 技术专家，简单，为人热情 "
}
```

在 Elasticsearch 中，每个字段的所有数据都是默认被索引的。即每个字段都有为了快速检索设置的专用倒排索引。它能在相同的查询中使用所有倒排索引，并以惊人的速度返回结果。

# 17 | 高性能缓存架构

在某些复杂的业务场景下，单纯依靠存储系统的性能提升不够的，典型的场景有：

- 需要经过复杂运算后得出的数据，存储系统无能为力

  例如，一个论坛需要在首页展示当前有多少用户同时在线。

- 读多写少的数据，存储系统有心无力

  以微博为例：一个明星发一条微博，可能几千万人来浏览。

缓存能够带来性能的大幅提升，以 Memcache 为例，单台 Memcache 服务器简单的 key-value 查询能够达到 TPS 50000 以上。

缓存虽然能够大大减轻存储系统的压力，但同时也给架构引入了更多复杂性。今天，我来逐一分析缓存的架构设计要点。

**缓存穿透**

缓存穿透是指缓存没有发挥作用，业务系统虽然去缓存查询数据，但缓存中没有数据，业务系统需要再次去存储系统查询数据。通常情况下有两种情况：

- 存储数据不存在

  第一种情况是被访问的数据确实不存在。用户查询的时候，在缓存中找不到对应的数据，每次都要去存储系统中再查询一遍，然后返回数据不存在。

  这种情况的解决办法比较简单，如果查询存储系统的数据没有找到，则直接设置一个默认值存到缓存中。

- 缓存数据生成耗费大量时间或者资源

  第二种情况是存储系统中存在数据，但生成缓存数据需要耗费较长时间或者耗费大量资源。如果刚好在业务访问的时候缓存失效了，那么也会出现缓存穿透。

**缓存雪崩**

缓存雪崩是指当缓存失效（过期）后引起系统性能急剧下降的情况。缓存雪崩的常见解决方法有两种：更新锁机制和后台更新机制。

- 更新锁

  对缓存更新操作进行加锁保护，保证只有一个线程能够进行缓存更新。如果是分布式集群的业务系统要实现更新锁机制，需要用到分布式锁。

- 后台更新

  由后台线程来更新缓存，而不是由业务线程来更新缓存，缓存本身的有效期设置为永久，后台线程定时更新缓存。

  后台更新缓存有两种方式：

  1. 定时更新缓存，且还要频繁地去读取缓存。
  2. 业务线程发现缓存失效后，通过消息队列发送消息通知后台线程更新缓存。

**缓存热点**

虽然缓存系统本身的性能比较高，但对于一些特别热点的数据，如果大部分甚至所有的业务请求都命中同一份缓存数据，则这份数据所在的缓存服务器的压力也很大。

缓存热点的解决方案就是复制多份缓存副本，将请求分散到多个缓存服务器上，减轻缓存热点导致的单台缓存服务器压力。

以微博为例，对于粉丝数超过 100 万的明星，每条微博都可以生成 100 份缓存，缓存的数据是一样的，通过在缓存的 key 里面加上编号进行区分，每次读缓存时都随机读取其中某份缓存。

不同的缓存副本不要设置统一的过期时间。正确的做法是设定一个过期时间范围，不同的缓存副本的过期时间是指定范围内的随机值。

# 18 | 单服务器高性能模式：PPC与TPC

架构设计决定了系统性能的上限，实现细节决定了系统性能的下限。今天，我们先来看看单服务器高性能模式：PPC 与 TPC。

**PPC**

PPC 是 Process Per Connection 的缩写，其含义是指每次有新的连接就新建一个进程去专门处理这个连接的请求，这是传统的 UNIX 网络服务器所采用的模型。

基本的流程图是：

<img src="https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210118223451.png" alt="image-20210118223451159" style="zoom:50%;" />

> 注意，图中有一个小细节，父进程“fork”子进程后，直接调用了 close，看起来好像是关闭了连接，其实只是将连接的文件描述符引用计数减一，真正的关闭连接是等子进程也调用 close 后，连接对应的文件描述符引用计数变为 0 后，操作系统才会真正关闭连接，更多细节请参考《UNIX 网络编程：卷一》。

PPC 模式实现简单，比较适合服务器的连接数没那么多的情况，例如数据库服务器。互联网兴起后，服务器的并发和访问量从几十剧增到成千上万，这种模式的弊端就凸显出来了，主要体现在这几个方面：

- fork 代价高
- 父子进程通信复杂
- 支持的并发连接数量有限

**prefork**

PPC 模式中，当连接进来时才 fork 新进程来处理连接请求，由于 fork 进程代价高，用户访问时可能感觉比较慢，prefork 模式的出现就是为了解决这个问题。

顾名思义，prefork 就是提前创建进程（pre-fork）。系统在启动的时候就预先创建好进程，然后才开始接受用户的请求，当有新的连接进来的时候，就可以省去 fork 进程的操作，让用户访问更快、体验更好。prefork 的基本示意图是：﻿﻿

<img src="https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210118224021.png" alt="image-20210118224021515" style="zoom:50%;" />

prefork 的实现关键就是多个子进程都 accept 同一个 socket，当有新的连接进入时，操作系统保证只有一个进程能最后 accept 成功。但这里也存在一个小小的问题：“惊群”现象，就是指虽然只有一个子进程能 accept 成功，但所有阻塞在 accept 上的子进程都会被唤醒，这样就导致了不必要的进程调度和上下文切换了。幸运的是，操作系统可以解决这个问题，例如 Linux 2.6 版本后内核已经解决了 accept 惊群问题。

prefork 模式和 PPC 一样，还是存在父子进程通信复杂、支持的并发连接数量有限的问题，因此目前实际应用也不多。Apache 服务器提供了 MPM prefork 模式，推荐在需要可靠性或者与旧软件兼容的站点时采用这种模式，默认情况下最大支持 256 个并发连接。

**TPC**

TPC 是 Thread Per Connection 的缩写，其含义是指每次有新的连接就新建一个线程去专门处理这个连接的请求。

TPC 实际上是解决或者弱化了 PPC fork 代价高的问题和父子进程通信复杂的问题。TPC 的基本流程是：

<img src="https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210118224931.png" alt="image-20210118224931623" style="zoom: 67%;" />

> 注意，和 PPC 相比，主进程不用“close”连接了。原因是在于子线程是共享主进程的进程空间的，连接的文件描述符并没有被复制，因此只需要一次 close 即可。

TPC 虽然解决了 fork 代价高和进程通信复杂的问题，但是也引入了新的问题，具体表现在：

- 高并发时（例如每秒上万连接）还是有性能问题
- 线程间的互斥和共享又引入了复杂度，可能一不小心就导致了死锁问题
- 多线程会出现互相影响的情况，某个线程出现异常时，可能导致整个进程退出（例如内存越界）

除了引入了新的问题，TPC 还是存在 CPU 线程调度和切换代价的问题。因此，TPC 方案本质上和 PPC 方案基本类似，在并发几百连接的场景下，反而更多地是采用 PPC 的方案，因为 PPC 方案不会有死锁的风险，也不会多进程互相影响，稳定性更高。

**prethread**

和 prefork 类似，prethread 模式会预先创建线程，然后才开始接受用户的请求，当有新的连接进来的时候，就可以省去创建线程的操作，让用户感觉更快、体验更好。

由于多线程之间数据共享和通信比较方便，因此实际上 prethread 的实现方式相比 prefork 要灵活一些，常见的实现方式有下面几种：

- 主进程 accept，然后将连接交给某个线程处理。
- 子线程都尝试去 accept，最终只有一个线程 accept 成功，方案的基本示意图如下：

<img src="https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210118225306.png" alt="image-20210118225306556" style="zoom:50%;" />

Apache 服务器的 MPM worker 模式本质上就是一种 prethread 方案，但稍微做了改进。Apache 服务器会首先创建多个进程，每个进程里面再创建多个线程，这样做主要是为了考虑稳定性，即：即使某个子进程里面的某个线程异常导致整个子进程退出，还会有其他子进程继续提供服务，不会导致整个服务器全部挂掉。

prethread 理论上可以比 prefork 支持更多的并发连接，Apache 服务器 MPM worker 模式默认支持 16 × 25 = 400 个并发处理线程。

# 19 | 单服务器高性能模式：Reactor与Proactor

PPC 和 TPC 在应对各种海量用户业务完全无能为力。今天我将介绍可以应对高并发场景的单服务器高性能架构模式：Reactor 和 Proactor。

**Reactor**

引入资源池的处理方式后，会引出一个新的问题：进程如何才能高效地处理多个连接的业务？

当一个连接一个进程时，进程可以采用“read -> 业务处理 -> write”的处理流程，如果当前连接没有数据可以读，则进程就阻塞在 read 操作上。如果一个进程处理多个连接，进程阻塞在某个连接的 read 操作上，此时即使其他连接有数据可读，进程也无法去处理，很显然这样是无法做到高性能的。

解决这个问题的最简单的方式是将 read 操作改为非阻塞，然后进程不断地轮询多个连接。但是，轮询是要消耗 CPU 的；其次，如果一个进程处理几千上万的连接，则轮询的效率是很低的。

为了能够更好地解决上述问题，很容易可以想到，只有当连接上有数据的时候进程才去处理，这就是 I/O 多路复用技术的来源。

I/O 多路复用技术归纳起来有两个关键实现点：

- 当多条连接共用一个阻塞对象后，进程只需要在一个阻塞对象上等待，而无须再轮询所有连接。常见的实现方式有 select、epoll、kqueue 等。
- 当某条连接有新的数据可以处理时，操作系统会通知进程，进程从阻塞状态返回，开始进行业务处理。

Reactor 模式的核心组成部分包括 Reactor 和处理资源池（进程池或线程池），其中 Reactor 负责监听和分配事件，处理资源池负责处理事件。

Reactor 模式有这三种典型的实现方案：

1. 单 Reactor 单进程 / 线程。

<img src="https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210119230630.png" alt="image-20210119230630227" style="zoom:50%;" />

> 注意，select、accept、read、send 是标准的网络编程 API，dispatch 和“业务处理”是需要完成的操作。

- Reactor 对象通过 select 监控连接事件，收到事件后通过 dispatch 进行分发。
- 如果是连接建立的事件，则由 Acceptor 处理，Acceptor 通过 accept 接受连接，并创建一个 Handler 来处理连接后续的各种事件。
- 如果不是连接建立事件，则 Reactor 会调用连接对应的 Handler（第 2 步中创建的 Handler）来进行响应。
- Handler 完成 read→业务处理→send 的完整业务流程。

单 Reactor 单进程的模式优点是：没有进程间通信。

缺点是：

- 只有一个进程，无法发挥多核 CPU 的性能。
- Handler 在处理某个连接上的业务时，整个进程无法处理其他连接的事件，很容易导致性能瓶颈。

因此，单 Reactor 单进程的方案在实践中应用场景不多，只适用于业务处理非常快速的场景，目前比较著名的开源软件中使用单 Reactor 单进程的是 Redis。

2. 单 Reactor 多线程。

<img src="https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210119231809.png" alt="image-20210119231809427" style="zoom: 67%;" />

- 主线程中，Reactor 对象通过 select 监控连接事件，收到事件后通过 dispatch 进行分发。
- 如果是连接建立的事件，则由 Acceptor 处理，Acceptor 通过 accept 接受连接，并创建一个 Handler 来处理连接后续的各种事件。
- 如果不是连接建立事件，则 Reactor 会调用连接对应的 Handler（第 2 步中创建的 Handler）来进行响应。
- Handler 只负责响应事件，不进行业务处理；Handler 通过 read 读取到数据后，会发给 Processor 进行业务处理。
- Processor 会在独立的子线程中完成真正的业务处理，然后将响应结果发给主进程的 Handler 处理；Handler 收到响应后通过 send 将响应结果返回给 client。

单 Reator 多线程方案能够充分利用多核多 CPU 的处理能力，但同时也存在下面的问题：

- 多线程数据共享和访问比较复杂。
- Reactor 承担所有事件的监听和响应，只在主线程中运行，瞬间高并发时会成为性能瓶颈。

> 为什么没有单 Reactor 多进程方案呢？
>
> 因为子进程完成业务处理后，将结果返回给父进程通信比较麻烦。

3. 多 Reactor 多进程 / 线程。

<img src="https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210119232355.png" alt="image-20210119232355825" style="zoom: 67%;" />

- 父进程中 mainReactor 对象通过 select 监控连接建立事件，收到事件后通过 Acceptor 接收，将新的连接分配给某个子进程。
- 子进程的 subReactor 将 mainReactor 分配的连接加入连接队列进行监听，并创建一个 Handler 用于处理连接的各种事件。
- 当有新的事件发生时，subReactor 会调用连接对应的 Handler（即第 2 步中创建的 Handler）来进行响应。
- Handler 完成 read→业务处理→send 的完整业务流程。

多 Reactor 多进程 / 线程的方案看起来比单 Reactor 多线程要复杂，但实际实现时反而更加简单，主要原因是：

- 父进程和子进程的职责非常明确，父进程只负责接收新连接，子进程负责完成后续的业务处理。
- 父进程和子进程的交互很简单，父进程只需要把新连接传给子进程，子进程无须返回数据。
- 子进程之间是互相独立的，无须同步共享之类的处理（这里仅限于网络模型相关的 select、read、send 等无须同步共享，“业务处理”还是有可能需要同步共享的）。

目前著名的开源系统 Nginx 采用的是多 Reactor 多进程，采用多 Reactor 多线程的实现有 Memcache 和 Netty。

**Proactor**

Reactor 是同步非阻塞网络模型，因为真正的 read 和 send 操作都需要用户进程同步操作。如果把 I/O 操作改为异步就能够进一步提升性能，这就是异步网络模型 Proactor。

Reactor 可以理解为“来了事件我通知你，你来处理”，而 Proactor 可以理解为“来了事件我来处理，处理完了我通知你”。这里的“我”就是操作系统内核，“事件”就是有新连接、有数据可读、有数据可写的这些 I/O 事件，“你”就是我们的程序代码。

Proactor 模型示意图是：

<img src="https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210119232908.png" alt="image-20210119232908323" style="zoom:67%;" />

- Proactor Initiator 负责创建 Proactor 和 Handler，并将 Proactor 和 Handler 都通过 Asynchronous Operation Processor 注册到内核。
- Asynchronous Operation Processor 负责处理注册请求，并完成 I/O 操作。
- Asynchronous Operation Processor 完成 I/O 操作后通知 Proactor。
- Proactor 根据不同的事件类型回调不同的 Handler 进行业务处理。
- Handler 完成业务处理，Handler 也可以注册新的 Handler 到内核进程。

理论上 Proactor 比 Reactor 效率要高一些，异步 I/O 能够充分利用 DMA 特性，让 I/O 操作与计算重叠，但要实现真正的异步 I/O，操作系统需要做大量的工作。

# 20 | 高性能负载均衡：分类及架构

高性能集群的复杂性主要体现在需要增加一个任务分配器，以及为任务选择一个合适的任务分配算法。

任务分配器又叫负载均衡器。实际上任务分配并不只是考虑计算单元的负载均衡，不同的任务分配算法目标是不一样的，有的基于负载考虑，有的基于性能（吞吐量、响应时间）考虑，有的基于业务考虑。

今天我先来讲讲负载均衡的分类及架构，下一期会讲负载均衡的算法。

常见的负载均衡系统包括 3 种：DNS 负载均衡、硬件负载均衡和软件负载均衡。

**DNS 负载均衡**

DNS 是最简单也是最常见的负载均衡方式，一般用来实现地理级别的均衡。例如，北方的用户访问北京的机房，南方的用户访问深圳的机房。

DNS 负载均衡的本质是 DNS 解析同一个域名可以返回不同的 IP 地址。例如，同样是 www.baidu.com，北方用户解析后获取的地址是 61.135.165.224（这是北京机房的 IP），南方用户解析后获取的地址是 14.215.177.38（这是深圳机房的 IP）。

下面是 DNS 负载均衡的简单示意图：

<img src="https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210120221200.png" alt="image-20210120221200293" style="zoom: 50%;" />

DNS 负载均衡优点有：

- 简单、成本低

  负载均衡工作交给 DNS 服务器处理，无须自己开发或者维护负载均衡设备。

- 就近访问，提升访问速度

  DNS 解析时可以根据请求来源 IP，解析成距离用户最近的服务器地址，可以加快访问速度，改善性能。

缺点有：

- 更新不及时

  DNS 缓存的时间比较长，修改 DNS 配置后，由于缓存的原因，还是有很多用户会继续访问修改前的 IP。

- 扩展性差

  DNS 负载均衡的控制权在域名商那里，无法根据业务特点针对其做更多的定制化功能和扩展特性。

- 分配策略比较简单

  DNS 负载均衡支持的算法少；不能区分服务器的差异（不能根据系统与服务的状态来判断负载）；也无法感知后端服务器的状态。

**硬件负载均衡**

硬件负载均衡是通过单独的硬件设备来实现负载均衡功能。目前业界典型的硬件负载均衡设备有两款：F5 和 A10。

硬件负载均衡的优点是：

- 功能强大

  全面支持各层级的负载均衡，支持全面的负载均衡算法，支持全局负载均衡。

- 性能强大

  对比一下，软件负载均衡支持到 10 万级并发已经很厉害了，硬件负载均衡可以支持 100 万以上的并发。

- 稳定性高

  商用硬件负载均衡，经过了良好的严格测试，经过大规模使用，稳定性高。

- 支持安全防护

  硬件均衡设备除具备负载均衡功能外，还具备防火墙、防 DDoS 攻击等安全功能。

硬件负载均衡的缺点是：

- 价格昂贵

  最普通的一台 F5 就是一台“马 6”，好一点的就是“Q7”了。

- 扩展能力差

  硬件设备，可以根据业务进行配置，但无法进行扩展和定制。

**软件负载均衡**

软件负载均衡通过负载均衡软件来实现负载均衡功能，常见的有 Nginx 和 LVS。

软件和硬件的最主要区别就在于性能，硬件负载均衡性能远远高于软件负载均衡性能。

- Ngxin 的性能是万级，一般的 Linux 服务器上装一个 Nginx 大概能到 5 万 / 秒；
- LVS 的性能是十万级，据说可达到 80 万 / 秒；
- F5 性能是百万级，从 200 万 / 秒到 800 万 / 秒都有。

下面是 Nginx 的负载均衡架构示意图：

<img src="https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210120221553.png" alt="image-20210120221553330" style="zoom:50%;" />

软件负载均衡的优点：

- 简单：无论是部署还是维护都比较简单。
- 便宜：只要买个 Linux 服务器，装上软件即可。
- 灵活：可以根据业务进行比较方便的扩展，例如，可以通过 Nginx 的插件来实现业务的定制化功能。

缺点是：

- 性能一般：一个 Nginx 大约能支撑 5 万并发。
- 功能没有硬件负载均衡那么强大。
- 一般不具备防火墙和防 DDoS 攻击等安全功能。

**负载均衡典型架构**

对于 3 种负载均衡机制：DNS 负载均衡、硬件负载均衡、软件负载均衡，实际使用时应当基于它们的优缺点进行组合使用。

组合的基本原则为：DNS 负载均衡用于实现地理级别的负载均衡；硬件负载均衡用于实现集群级别的负载均衡；软件负载均衡用于实现机器级别的负载均衡。

<img src="https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210120222240.png" alt="image-20210120221733978" style="zoom: 67%;" />

上图只是一个示例，一般在大型业务场景下才会这样用，如果业务量没这么大，则没有必要严格照搬这套架构。例如，一个大学的论坛，完全可以不需要 DNS 负载均衡，也不需要 F5 设备，只需要用 Nginx 作为一个简单的负载均衡就足够了。

**思考题**

假设你来设计一个日活跃用户 1000 万的论坛的负载均衡集群，你的方案是什么？设计理由是什么？

```
1、首先，流量评估。

1000 万 DAU，换算成秒级，平均约等于116。考虑每个用户操作次数，假定10，换算成平均QPS=1160。考虑峰值是均值倍数，假定10，换算成峰值QPS=11600。考虑静态资源、图片资源、服务拆分等，流量放大效应，假定10，QPS\*10=116000。

2、其次，容量规划。

考虑高可用、异地多活，QPS\*2=232000。考虑未来半年增长，QPS\*1.5=348000。

3、最后，方案设计。

三级导流。第一级，DNS，确定机房，以目前量级，可以不考虑。第二级，确定集群，扩展优先，则选Haproxy/LVS，稳定优先则选 F5。第三级，Nginx+KeepAlived，确定实例。
```

# 21 | 高性能负载均衡：算法

根据算法期望达到的目的，大体上可以分为下面几类。

- 任务平分类
- 负载均衡类
- 性能最优类
- Hash 类

接下来我介绍一下负载均衡算法以及它们的优缺点。

**轮询**

负载均衡系统收到请求后，按照顺序轮流分配到服务器上。

轮询是最简单的一个策略，无须关注服务器本身的状态。但如果服务器直接宕机了，或者服务器和负载均衡系统断连了，这时负载均衡系统是能够感知的，也需要做出相应的处理

**加权轮询**

负载均衡系统根据服务器权重进行任务分配。加权轮询是轮询的一种特殊形式，其主要目的就是为了解决不同服务器处理能力有差异的问题。

加权轮询解决了轮询算法中无法根据服务器的配置差异进行任务分配的问题，但同样存在无法根据服务器的状态差异进行任务分配的问题。

**负载最低优先**

负载均衡系统将任务分配给当前负载最低的服务器。负载最低优先的算法解决了轮询算法中无法感知服务器状态的问题，由此带来的代价是复杂度要增加很多。

**性能最优类**

负载最低优先类算法是站在服务器的角度来进行分配的，而性能最优优先类算法则是站在客户端的角度来进行分配的，优先将任务分配给处理速度最快的服务器，通过这种方式达到最快响应客户端的目的。

性能最优优先类算法本质上也是感知了服务器的状态，只是通过响应时间这个外部标准来衡量服务器状态而已。

**Hash 类**

负载均衡系统根据任务中的某些关键信息进行 Hash 运算，将相同 Hash 值的请求分配到同一台服务器上，这样做的目的主要是为了满足特定的业务需求。例如：

- 源地址 Hash

将来源于同一个源 IP 地址的任务分配给同一个服务器进行处理，适合于存在事务、会话的业务。

- ID Hash

将某个 ID 标识的业务分配到同一个服务器中进行处理。

**思考题**

微信抢红包的高并发架构，应该采取什么样的负载均衡算法？

微信抢红包架构应该至少包含两个负载均衡，一个是应用服务器的负载均衡，用于将任务请求分发到不同应用服务器，这里可以采用轮询或加速轮询的算法，因为这种速度快，适合抢红包的业务场景;

另一起负载均衡是数据服务器的负载均衡，这里更适合根据红包ID进行hash负载均衡，将所有数据请求在同一台服务器上进行，防止多台服务器间的不同步问题。