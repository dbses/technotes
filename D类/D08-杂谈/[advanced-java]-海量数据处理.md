> 来源：https://github.com/doocs/advanced-java/blob/master/docs/big-data/find-common-urls.md

# 01 | 如何从大量的 URL 中找出相同的 URL？

**题目描述**

给定 a、b 两个文件，各存放 50 亿个 URL，每个 URL 各占 64Byte，内存限制是 4G。请找出 a、b 两个文件共同的 URL。

**解答思路**

每个 URL 占 64B，那么 50 亿个 URL 占用的空间大小约为 320GB。

> 5,000,000,000 * 64Byte = 4.65GB * 64 = 298GB

由于内存大小只有 4G，因此，我们不可能一次性把所有 URL 加载到内存中处理。对于这种类型的题目，一般采用**分治策略**，即：把一个文件中的 URL 按照某个特征划分为多个小文件，使得每个小文件大小不超过 4G，这样就可以把这个小文件读到内存中进行处理了。

思路如下：

首先遍历文件 a，对遍历到的 URL 求 `hash(URL) % 1000` ，根据计算结果把遍历到的 URL 存储到 a0, a1, a2, ..., a999，这样每个大小约为 300MB。使用同样的方法遍历文件 b，把文件 b 中的 URL 分别存储到文件 b0, b1, b2, ..., b999 中。这样处理过后，所有可能相同的 URL 都在对应的小文件中，即 a0 对应 b0, ..., a999 对应 b999，不对应的小文件不可能有相同的 URL。那么接下来，我们只需要求出这 1000 对小文件中相同的 URL 就好了。

接着遍历 ai( `i∈[0,999]` )，把 URL 存储到一个 HashSet 集合中。然后遍历 bi 中每个 URL，看在 HashSet 集合中是否存在，若存在，说明这就是共同的 URL，可以把这个 URL 保存到一个单独的文件中。

**方法总结**

1. 分而治之，进行哈希取余；
2. 对每个子文件进行 HashSet 统计。

# 02 | 如何从大量数据中找出高频词？

**题目描述**

有一个 1GB 大小的文件，文件里每一行是一个词，每个词的大小不超过 16B，内存大小限制是 1MB，要求返回频数最高的 100 个词(Top 100)。

**解答思路**

由于内存限制，我们依然无法直接将大文件的所有词一次读到内存中。因此，同样可以采用**分治策略**，把一个大文件分解成多个小文件，保证每个文件的大小小于 1MB，进而直接将单个小文件读取到内存中进行处理。

思路如下：

首先遍历大文件，对遍历到的每个词 x，执行 `hash(x) % 5000` ，将结果为 i 的词存放到文件 ai 中。遍历结束后，我们可以得到 5000 个小文件。每个小文件的大小为 200KB 左右。如果有的小文件大小仍然超过 1MB，则采用同样的方式继续进行分解。

接着统计每个小文件中出现频数最高的 100 个词。最简单的方式是使用 HashMap 来实现。其中 key 为词，value 为该词出现的频率。具体方法是：对于遍历到的词 x，如果在 map 中不存在，则执行 `map.put(x, 1)` ；若存在，则执行 `map.put(x, map.get(x)+1)` ，将该词频数加 1。

上面我们统计了每个小文件单词出现的频数。接下来，我们可以通过维护一个**小顶堆**来找出所有词中出现频数最高的 100 个。具体方法是：依次遍历每个小文件，构建一个**小顶堆**，堆大小为 100。如果遍历到的词的出现次数大于堆顶词的出现次数，则用新词替换堆顶的词，然后重新调整为**小顶堆**，遍历结束后，小顶堆上的词就是出现频数最高的 100 个词。

**方法总结**

1. 分而治之，进行哈希取余；
2. 使用 HashMap 统计频数；
3. 求解**最大**的 TopN 个，用**小顶堆**；求解**最小**的 TopN 个，用**大顶堆**。

# 03 | 如何找出某一天访问百度网站最多的 IP？

**题目描述**

现有海量日志数据保存在一个超大文件中，该文件无法直接读入内存，要求从中提取某天访问百度次数最多的那个 IP。

**解答思路**

这道题只关心某一天访问百度最多的 IP，因此，可以首先对文件进行一次遍历，把这一天访问百度 IP 的相关信息记录到一个单独的大文件中。接下来采用的方法与上一题一样，大致就是先对 IP 进行哈希映射，接着使用 HashMap 统计重复 IP 的次数，最后计算出重复次数最多的 IP。

> 注：这里只需要找出出现次数最多的 IP，可以不必使用堆，直接用一个变量 max 即可。

**方法总结**

1. 分而治之，进行哈希取余；
2. 使用 HashMap 统计频数；
3. 求解**最大**的 TopN 个，用**小顶堆**；求解**最小**的 TopN 个，用**大顶堆**。

# 04 | 如何在大量的数据中找出不重复的整数？

**题目描述**

在 2.5 亿个整数中找出不重复的整数。

> 注意：内存不足以容纳这 2.5 亿个整数。

**解答思路**

- 方法一：分治法

与前面的题目方法类似，先将 2.5 亿个数划分到多个小文件，用 HashSet/HashMap 找出每个小文件中不重复的整数，再合并每个子结果，即为最终结果。

- 方法二：位图法

对于整数相关的算法的求解，位图法是一种非常实用的算法。假设 int 整数占用 4B，即 32bit，那么我们可以表示的整数的个数为 232。

**那么对于这道题**，我们用 2 个 bit 来表示各个数字的状态：

- 00 表示这个数字没出现过；
- 01 表示这个数字出现过一次（即为题目所找的不重复整数）；
- 10 表示这个数字出现了多次。

那么这 232 个整数，总共所需内存为 232*2b=1GB。因此，当可用内存超过 1GB 时，可以采用位图法。假设内存满足位图法需求，进行下面的操作：

遍历 2.5 亿个整数，查看位图中对应的位，如果是 00，则变为 01，如果是 01 则变为 10，如果是 10 则保持不变。遍历结束后，查看位图，把对应位是 01 的整数输出即可。

```text
0 0 0 0 0 0 0...0
0 0 0 0 0 0 0...0 (2.5亿个)
```

假如这2.5亿个整数为[6, 5, 1, 1 ...]

那么可表示为：

```text
0 1 0 0 0 0 0...0
0 0 0 0 0 1 1...0 (2.5亿个)
```

**方法总结**

判断数字是否重复的问题，位图法是一种非常高效的方法。

# 05 | 如何在大量的数据中判断一个数是否存在？

**题目描述**

给定 40 亿个不重复的没排过序的 unsigned int 型整数，然后再给定一个数，如何快速判断这个数是否在这 40 亿个整数当中？

**解答思路**

40 亿个不重复整数，我们用 40 亿个 bit 来表示，初始位均为 0，那么总共需要内存：4, 000, 000, 000b≈512M。

我们读取这 40 亿个整数，将对应的 bit 设置为 1。接着读取要查询的数，查看相应位是否为 1，如果为 1 表示存在，如果为 0 表示不存在。

**方法总结**

判断数字是否存在、判断数字是否重复的问题，位图法是一种非常高效的方法。

**实现样例**

> 来源：https://blog.csdn.net/qfzxhy/article/details/54951472

```java
// 对16以下的数进行排序以及统计不同数字个数
public class BitGraph {
    final int BITS_PRE_WORD = 32;
    final static int max = 16;
    void setBit(int[] arr, int n) {
        arr[n/BITS_PRE_WORD] |= (1 << (n % BITS_PRE_WORD));
    }
    void clearBit(int[] arr, int n) {}
    int getBit(int[] arr, int n) {
        return (arr[n/BITS_PRE_WORD] & (1 << (n%BITS_PRE_WORD))) != 0 ? 1 : 0;
    }
    public static void main(String[] args) {
        // TODO Auto-generated method stub
        BitGraph bg = new BitGraph();
        int[] datas = new int[] {1,13,14,15,7,8,9,13,1,13,14,15,7,8,9,13,2};
        int[] arr = new int[max / 32 + 1];
        for(int data : datas) {
            bg.setBit(arr, data);
        }
        int count = 0;
        for(int i = 0; i < max;i++) {
            if(bg.getBit(arr, i) == 1) {
                System.out.println(i);
                ++count;
            }
        }
        System.out.println("count" + count);
    }
}
```

# 06 | 如何从 5 亿个数中找出中位数？

**题目描述**

从 5 亿个数中找出中位数。数据排序后，位置在最中间的数就是中位数。当样本数为奇数时，中位数为 第 `(N+1)/2` 个数；当样本数为偶数时，中位数为 第 `N/2` 个数与第 `1+N/2` 个数的均值。

**解答思路**

如果这道题没有内存大小限制，则可以把所有数读到内存中排序后找出中位数。但是最好的排序算法的时间复杂度都为 `O(NlogN)` 。这里使用其他方法。

分治法的思想是把一个大的问题逐渐转换为规模较小的问题来求解。

对于这道题，顺序读取这 5 亿个数字，对于读取到的数字 num，如果它对应的二进制中最高位为 1，则把这个数字写到 f1 中，否则写入 f0 中。通过这一步，可以把这 5 亿个数划分为两部分，而且 f0 中的数都大于 f1 中的数（最高位是符号位）。

划分之后，可以非常容易地知道中位数是在 f0 还是 f1 中。假设 f1 中有 1 亿个数，那么中位数一定在 f0 中，且是在 f0 中，从小到大排列的第 2 亿个数与它后面的一个数的平均值。

> **提示**，5 亿数的中位数是第 2.5 亿与右边相邻一个数求平均值。若 f1 有一亿个数，那么中位数就是 f0 中从第 2 亿个数开始的两个数求得的平均值。

对于 f0 可以用次高位的二进制继续将文件一分为二，如此划分下去，直到划分后的文件可以被加载到内存中，把数据加载到内存中以后直接排序，找出中位数。

> **注意**，当数据总数为偶数，如果划分后两个文件中的数据有相同个数，那么中位数就是数据较小的文件中的最大值与数据较大的文件中的最小值的平均值。

**方法总结**

分治法。